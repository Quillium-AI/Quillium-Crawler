# Quillium-Crawler Example Environment File

# Required: At least one of these must be set
CRAWLER_START_URL=https://example.com              # Single start URL (used if CRAWLER_START_URLS is not set)
CRAWLER_START_URLS=https://example.com,https://example2.com  # Comma-separated list of start URLs (each will spawn a crawler)

# Depth and limits
CRAWLER_MAX_DEPTH=3                               # Maximum crawl depth
CRAWLER_MAX_VISITS=1000                           # Maximum number of pages to visit per crawler

# Parallelism and delays
CRAWLER_PARALLEL_REQUESTS=10                      # Number of parallel requests per crawler
CRAWLER_DELAY_MS=50                               # Delay between requests in milliseconds
CRAWLER_RANDOM_DELAY_MS=50                        # Random delay added to each request in milliseconds
CRAWLER_TIMEOUT_SEC=10                            # Request timeout in seconds

# Domain and URL filtering
CRAWLER_ALLOWED_DOMAINS=example.com,example2.com  # Only crawl these domains (comma-separated, empty for all)
CRAWLER_DISALLOWED_DOMAINS=                       # Domains to skip (comma-separated)
CRAWLER_ALLOWED_URLS=                             # Only crawl these URLs (comma-separated, optional)
CRAWLER_DISALLOWED_URLS=                          # URLs to skip (comma-separated, optional)
CRAWLER_IGNORE_QUERY_STRINGS=false                # Ignore query strings in URLs (true/false)
CRAWLER_RESPECT_ROBOTS_TXT=true                   # Whether to respect robots.txt rules (true/false)

# Output
CRAWLER_OUTPUT_FILE=crawled_data.json             # Path to output file
CRAWLER_ENABLE_FULL_CONTENT=false                # Enable full page content scraping (true/false)

# Metrics
CRAWLER_ENABLE_METRICS=true                     # Enable Prometheus metrics endpoint (true/false)

# Proxies
CRAWLER_PROXIES=http://user:pass@proxyserver:8080 # Comma-separated list of proxies (optional)

# Anti-bot measures
CRAWLER_ENABLE_USER_AGENT_ROTATION=true      # Enable random user agent rotation
CRAWLER_ENABLE_HEADER_RANDOMIZATION=true     # Enable HTTP header randomization
CRAWLER_ENABLE_COOKIE_HANDLING=true          # Enable browser-like cookie handling
CRAWLER_ENABLE_SOPHISTICATED_DELAYS=true     # Enable more human-like delays
CRAWLER_RANDOM_DELAY_FACTOR=1.5              # Factor for random delay calculation (0.5-2.0 recommended)
CRAWLER_CUSTOM_USER_AGENTS=                  # Additional custom user agents (comma-separated)
CRAWLER_CUSTOM_ACCEPT_LANGUAGES=             # Additional custom accept-language values (comma-separated)
